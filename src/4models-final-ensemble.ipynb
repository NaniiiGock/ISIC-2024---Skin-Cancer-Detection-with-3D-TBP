{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":63056,"databundleVersionId":9094797,"sourceType":"competition"},{"sourceId":10200667,"sourceType":"datasetVersion","datasetId":6303449},{"sourceId":10214379,"sourceType":"datasetVersion","datasetId":6313340}],"dockerImageVersionId":30805,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-12-14T18:02:10.670158Z","iopub.execute_input":"2024-12-14T18:02:10.670531Z","iopub.status.idle":"2024-12-14T18:02:11.969851Z","shell.execute_reply.started":"2024-12-14T18:02:10.670496Z","shell.execute_reply":"2024-12-14T18:02:11.968884Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Tabular Data","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/isic-2024-challenge/train-metadata.csv', low_memory=False)\ntest_data = pd.read_csv('/kaggle/input/isic-2024-challenge/test-metadata.csv', low_memory=False)","metadata":{"execution":{"iopub.status.busy":"2024-12-14T18:02:11.971420Z","iopub.execute_input":"2024-12-14T18:02:11.971875Z","iopub.status.idle":"2024-12-14T18:02:21.689057Z","shell.execute_reply.started":"2024-12-14T18:02:11.971841Z","shell.execute_reply":"2024-12-14T18:02:21.687829Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"useless_cols = set(train_data.columns) - set(test_data.columns)\nuseless_cols.remove('target')\nuseless_cols = list(useless_cols)\ntrain_data = train_data.drop(columns=useless_cols)\ntrain_size = train_data.shape[0]","metadata":{"execution":{"iopub.status.busy":"2024-12-14T18:02:21.690266Z","iopub.execute_input":"2024-12-14T18:02:21.690577Z","iopub.status.idle":"2024-12-14T18:02:21.781723Z","shell.execute_reply.started":"2024-12-14T18:02:21.690546Z","shell.execute_reply":"2024-12-14T18:02:21.780821Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"X_train = train_data.drop(columns='target')\ny_train = train_data['target']\nX_train.fillna({'age_approx': X_train['age_approx'].mean(),'anatom_site_general':'NA','sex':'NA'}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-12-14T18:02:21.783558Z","iopub.execute_input":"2024-12-14T18:02:21.784050Z","iopub.status.idle":"2024-12-14T18:02:21.940299Z","shell.execute_reply.started":"2024-12-14T18:02:21.783989Z","shell.execute_reply":"2024-12-14T18:02:21.939119Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"all_data = pd.concat([X_train, test_data])\nall_data.reset_index(drop=True, inplace=True)\nall_data = all_data.drop(columns=['copyright_license', 'attribution', 'image_type'])\n# For second xgb model\nall_data_2 = all_data.copy()","metadata":{"execution":{"iopub.status.busy":"2024-12-14T18:02:21.941502Z","iopub.execute_input":"2024-12-14T18:02:21.941804Z","iopub.status.idle":"2024-12-14T18:02:22.119951Z","shell.execute_reply.started":"2024-12-14T18:02:21.941775Z","shell.execute_reply":"2024-12-14T18:02:22.118942Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, OneHotEncoder\ncol_num = []\ncol_cat = []\nfor col in all_data:\n  if col in ['isic_id', 'patient_id']: continue;\n  if all_data[col].dtype == 'object' or all_data[col].dtype == 'character':\n    col_cat.append(col)\n    all_data[col] = all_data[col].astype('category')\n  else:\n    all_data[col] = all_data[col].astype('float64')\n    col_num.append(col)","metadata":{"execution":{"iopub.status.busy":"2024-12-14T18:02:22.121353Z","iopub.execute_input":"2024-12-14T18:02:22.121770Z","iopub.status.idle":"2024-12-14T18:02:23.653476Z","shell.execute_reply.started":"2024-12-14T18:02:22.121714Z","shell.execute_reply":"2024-12-14T18:02:23.652353Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Normalization for numeric values\nnumeric_transformer = StandardScaler()\nall_data.loc[:,col_num] = numeric_transformer.fit_transform(all_data[col_num])","metadata":{"execution":{"iopub.status.busy":"2024-12-14T18:02:23.655124Z","iopub.execute_input":"2024-12-14T18:02:23.655780Z","iopub.status.idle":"2024-12-14T18:02:24.060434Z","shell.execute_reply.started":"2024-12-14T18:02:23.655729Z","shell.execute_reply":"2024-12-14T18:02:24.058986Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"categorical_transformer = OneHotEncoder(sparse_output=False)\nX_cat = categorical_transformer.fit_transform(all_data[col_cat])\nnew_cat_col = categorical_transformer.get_feature_names_out(col_cat)","metadata":{"execution":{"iopub.status.busy":"2024-12-14T18:02:24.061756Z","iopub.execute_input":"2024-12-14T18:02:24.062128Z","iopub.status.idle":"2024-12-14T18:02:24.701051Z","shell.execute_reply.started":"2024-12-14T18:02:24.062093Z","shell.execute_reply":"2024-12-14T18:02:24.699979Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"all_data = all_data.drop(columns=col_cat)\nall_data = pd.concat([all_data,pd.DataFrame(X_cat, columns=new_cat_col)],axis=1)\nX_train = all_data.iloc[0:train_size,:]\nX_test = all_data.iloc[train_size:,:]\nX_test.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-12-14T18:02:24.702403Z","iopub.execute_input":"2024-12-14T18:02:24.702711Z","iopub.status.idle":"2024-12-14T18:02:25.190270Z","shell.execute_reply.started":"2024-12-14T18:02:24.702674Z","shell.execute_reply":"2024-12-14T18:02:25.188984Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"col_id = ['isic_id', 'patient_id']\nX_train_id = X_train[col_id]\nX_train_data = X_train.drop(columns=col_id)\nX_test_id = X_test[col_id]\nX_test_data = X_test.drop(columns=col_id)","metadata":{"execution":{"iopub.status.busy":"2024-12-14T18:02:25.195218Z","iopub.execute_input":"2024-12-14T18:02:25.195592Z","iopub.status.idle":"2024-12-14T18:02:25.281544Z","shell.execute_reply.started":"2024-12-14T18:02:25.195559Z","shell.execute_reply":"2024-12-14T18:02:25.280327Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import xgboost as xgb\nimport seaborn as sns\n\nneg, pos = np.bincount(y_train)\nscale_pos_weight = neg/pos\nxgb_ml = xgb.XGBClassifier(\n            scale_pos_weight=scale_pos_weight,  # Handle class imbalance\n            learning_rate=0.01,\n            n_estimators=200,\n            max_depth=3,\n            min_child_weight=4,\n            gamma=1,\n            subsample=0.8,\n            colsample_bytree=0.8,\n            random_state=42,\n            eval_metric=['logloss','aucpr']  # Use appropriate metric for binary classification\n        )","metadata":{"execution":{"iopub.status.busy":"2024-12-14T18:02:25.282934Z","iopub.execute_input":"2024-12-14T18:02:25.283325Z","iopub.status.idle":"2024-12-14T18:02:26.186869Z","shell.execute_reply.started":"2024-12-14T18:02:25.283290Z","shell.execute_reply":"2024-12-14T18:02:26.185818Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# from sklearn.decomposition import PCA\n# pca = PCA(n_components=41)\n# X_train_pca = pca.fit_transform(X_train_data)\n# X_test_pca = pca.transform(X_test_data)\n# xgb_ml.fit(X_train_pca, y_train)\n# predictions = xgb_ml.predict_proba(X_test_pca)[:, 1]\n\n# def features_todrop(df, threshold=0.8):\n#     corr_matrix = df.corr().abs()\n#     upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n#     return  [column for column in upper.columns if any(upper[column] > threshold)]\n\n# to_drop = features_todrop(X_train_data[col_num])\n# X_train_data = X_train_data.drop(to_drop, axis=1)\n# X_test_data = X_test_data.drop(to_drop, axis=1)\nxgb_ml.fit(X_train_data, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-12-14T18:02:26.188424Z","iopub.execute_input":"2024-12-14T18:02:26.189327Z","iopub.status.idle":"2024-12-14T18:02:34.390391Z","shell.execute_reply.started":"2024-12-14T18:02:26.189274Z","shell.execute_reply":"2024-12-14T18:02:34.389251Z"},"trusted":true},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=0.75, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=['logloss', 'aucpr'],\n              feature_types=None, gamma=1, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=0.01, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=3,\n              max_leaves=None, min_child_weight=4, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=200,\n              n_jobs=None, num_parallel_tree=None, random_state=42, ...)","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=0.75, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=[&#x27;logloss&#x27;, &#x27;aucpr&#x27;],\n              feature_types=None, gamma=1, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=0.01, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=3,\n              max_leaves=None, min_child_weight=4, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=200,\n              n_jobs=None, num_parallel_tree=None, random_state=42, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=0.75, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=[&#x27;logloss&#x27;, &#x27;aucpr&#x27;],\n              feature_types=None, gamma=1, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=0.01, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=3,\n              max_leaves=None, min_child_weight=4, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=200,\n              n_jobs=None, num_parallel_tree=None, random_state=42, ...)</pre></div></div></div></div></div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"all_data_2[\"lesion_size_ratio\"] = all_data_2[\"tbp_lv_minorAxisMM\"] / all_data_2[\"clin_size_long_diam_mm\"]\nall_data_2[\"lesion_shape_index\"] = all_data_2[\"tbp_lv_areaMM2\"] / (all_data_2[\"tbp_lv_perimeterMM\"] ** 2)\nall_data_2[\"hue_contrast\"] = (all_data_2[\"tbp_lv_H\"] - all_data_2[\"tbp_lv_Hext\"]).abs()\nall_data_2[\"luminance_contrast\"] = (all_data_2[\"tbp_lv_L\"] - all_data_2[\"tbp_lv_Lext\"]).abs()\nall_data_2[\"lesion_color_difference\"] = np.sqrt(all_data_2[\"tbp_lv_deltaA\"] ** 2 + all_data_2[\"tbp_lv_deltaB\"] ** 2 + all_data_2[\"tbp_lv_deltaL\"] ** 2)\nall_data_2[\"border_complexity\"] = all_data_2[\"tbp_lv_norm_border\"] + all_data_2[\"tbp_lv_symm_2axis\"]\nall_data_2[\"3d_position_distance\"] = np.sqrt(all_data_2[\"tbp_lv_x\"] ** 2 + all_data_2[\"tbp_lv_y\"] ** 2 + all_data_2[\"tbp_lv_z\"] ** 2)\nall_data_2[\"perimeter_to_area_ratio\"] = all_data_2[\"tbp_lv_perimeterMM\"] / all_data_2[\"tbp_lv_areaMM2\"]\nall_data_2[\"area_to_perimeter_ratio\"] = all_data_2[\"tbp_lv_areaMM2\"] / all_data_2[\"tbp_lv_perimeterMM\"]\nall_data_2[\"lesion_visibility_score\"] = all_data_2[\"tbp_lv_deltaLBnorm\"] + all_data_2[\"tbp_lv_norm_color\"]\nall_data_2[\"combined_anatomical_site\"] = all_data_2[\"anatom_site_general\"] + \"_\" + all_data_2[\"tbp_lv_location\"]\nall_data_2[\"symmetry_border_consistency\"] = all_data_2[\"tbp_lv_symm_2axis\"] * all_data_2[\"tbp_lv_norm_border\"]\nall_data_2[\"consistency_symmetry_border\"] = all_data_2[\"tbp_lv_symm_2axis\"] * all_data_2[\"tbp_lv_norm_border\"] / (all_data_2[\"tbp_lv_symm_2axis\"] + all_data_2[\"tbp_lv_norm_border\"])\nall_data_2[\"color_consistency\"] = all_data_2[\"tbp_lv_stdL\"] / all_data_2[\"tbp_lv_Lext\"]\nall_data_2[\"consistency_color\"] = all_data_2[\"tbp_lv_stdL\"] * all_data_2[\"tbp_lv_Lext\"] / (all_data_2[\"tbp_lv_stdL\"] + all_data_2[\"tbp_lv_Lext\"])\nall_data_2[\"size_age_interaction\"] = all_data_2[\"clin_size_long_diam_mm\"] * all_data_2[\"age_approx\"]\nall_data_2[\"hue_color_std_interaction\"] = all_data_2[\"tbp_lv_H\"] * all_data_2[\"tbp_lv_color_std_mean\"]\nall_data_2[\"lesion_severity_index\"] = (all_data_2[\"tbp_lv_norm_border\"] + all_data_2[\"tbp_lv_norm_color\"] + all_data_2[\"tbp_lv_eccentricity\"]) / 3\nall_data_2[\"shape_complexity_index\"] = all_data_2[\"border_complexity\"] + all_data_2[\"lesion_shape_index\"]\n\n# all_data_2[\"color_contrast_index\"] = all_data_2[\"tbp_lv_deltaA\"] + all_data_2[\"tbp_lv_deltaB\"] + all_data_2[\"tbp_lv_deltaL\"] + all_data_2[\"tbp_lv_deltaLBnorm\"]\n# all_data_2[\"log_lesion_area\"] = np.log(all_data_2[\"tbp_lv_areaMM2\"] + 1)\n# all_data_2[\"normalized_lesion_size\"] = all_data_2[\"clin_size_long_diam_mm\"] / all_data_2[\"age_approx\"]\n# all_data_2[\"mean_hue_difference\"] = (all_data_2[\"tbp_lv_H\"] + all_data_2[\"tbp_lv_Hext\"]) / 2\n# all_data_2[\"std_dev_contrast\"] = np.sqrt((all_data_2[\"tbp_lv_deltaA\"] ** 2 + all_data_2[\"tbp_lv_deltaB\"] ** 2 + all_data_2[\"tbp_lv_deltaL\"] ** 2) / 3)\n# all_data_2[\"color_shape_composite_index\"] = (all_data_2[\"tbp_lv_color_std_mean\"] + all_data_2[\"tbp_lv_area_perim_ratio\"] + all_data_2[\"tbp_lv_symm_2axis\"]) / 3\n# all_data_2[\"3d_lesion_orientation\"] = np.arctan2(all_data_2[\"tbp_lv_y\"], all_data_2[\"tbp_lv_x\"])\n# all_data_2[\"overall_color_difference\"] = (all_data_2[\"tbp_lv_deltaA\"] + all_data_2[\"tbp_lv_deltaB\"] + all_data_2[\"tbp_lv_deltaL\"]) / 3\n# all_data_2[\"symmetry_perimeter_interaction\"] = all_data_2[\"tbp_lv_symm_2axis\"] * all_data_2[\"tbp_lv_perimeterMM\"]\n# all_data_2[\"comprehensive_lesion_index\"] = (all_data_2[\"tbp_lv_area_perim_ratio\"] + all_data_2[\"tbp_lv_eccentricity\"] + all_data_2[\"tbp_lv_norm_color\"] + all_data_2[\"tbp_lv_symm_2axis\"]) / 4\n# all_data_2[\"color_variance_ratio\"] = all_data_2[\"tbp_lv_color_std_mean\"] / all_data_2[\"tbp_lv_stdLExt\"]\n# all_data_2[\"border_color_interaction\"] = all_data_2[\"tbp_lv_norm_border\"] * all_data_2[\"tbp_lv_norm_color\"]\n# all_data_2[\"border_color_interaction_2\"] = all_data_2[\"tbp_lv_norm_border\"] * all_data_2[\"tbp_lv_norm_color\"] / (all_data_2[\"tbp_lv_norm_border\"] + all_data_2[\"tbp_lv_norm_color\"])\n# all_data_2[\"size_color_contrast_ratio\"] = all_data_2[\"clin_size_long_diam_mm\"] / all_data_2[\"tbp_lv_deltaLBnorm\"]\n# all_data_2[\"age_normalized_nevi_confidence\"] = all_data_2[\"tbp_lv_nevi_confidence\"] / all_data_2[\"age_approx\"]\n# all_data_2[\"age_normalized_nevi_confidence_2\"] = np.sqrt(all_data_2[\"clin_size_long_diam_mm\"]**2 + all_data_2[\"age_approx\"]**2)\n# all_data_2[\"color_asymmetry_index\"] = all_data_2[\"tbp_lv_radial_color_std_max\"] * all_data_2[\"tbp_lv_symm_2axis\"]\n# all_data_2[\"volume_approximation_3d\"] = all_data_2[\"tbp_lv_areaMM2\"] * np.sqrt((all_data_2[\"tbp_lv_x\"]**2 + all_data_2[\"tbp_lv_y\"]**2 + all_data_2[\"tbp_lv_z\"]**2))\n# all_data_2[\"color_range \"] = (all_data_2[\"tbp_lv_L\"] - all_data_2[\"tbp_lv_Lext\"]).abs() + (all_data_2[\"tbp_lv_A\"] - all_data_2[\"tbp_lv_Aext\"]).abs() + (all_data_2[\"tbp_lv_B\"] - all_data_2[\"tbp_lv_Bext\"]).abs()\n# all_data_2[\"shape_color_consistency\"] = all_data_2[\"tbp_lv_eccentricity\"] * all_data_2[\"tbp_lv_color_std_mean\"]\n# all_data_2[\"border_length_ratio\"] = all_data_2[\"tbp_lv_perimeterMM\"] / (2 * np.pi * np.sqrt(all_data_2[\"tbp_lv_areaMM2\"] / np.pi))\n# all_data_2[\"age_size_symmetry_index\"] = all_data_2[\"age_approx\"] * all_data_2[\"clin_size_long_diam_mm\"] * all_data_2[\"tbp_lv_symm_2axis\"]\n# all_data_2[\"index_age_size_symmetry\"] = all_data_2[\"age_approx\"] * all_data_2[\"tbp_lv_areaMM2\"] * all_data_2[\"tbp_lv_symm_2axis\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_data_2 = all_data_2.drop(columns=col_id)\nX_train_2 = all_data_2.iloc[0:train_size,:]\nX_test_2 = all_data_2.iloc[train_size:,:]\nX_test_2.reset_index(drop=True, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nnum_transformer = Pipeline(steps=[\n    ('scaler', StandardScaler())\n])\ncat_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', num_transformer, col_num),\n        ('cat', cat_transformer, col_cat)\n    ]\n)\n\nbest_xgb_params = {\n    'colsample_bytree': 0.8659,\n    'gamma': 0.1787,\n    'learning_rate': 0.0214,\n    'max_depth': int(4.9),  # Convert to int as required by XGBoost\n    'min_child_weight': 17.38,\n    'n_estimators': int(130.67),  # Convert to int as required by XGBoost\n    'reg_alpha': 0.8864,\n    'reg_lambda': 0.3854,\n    'subsample': 0.9212,\n    'random_state': 42\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom imblearn.over_sampling import SMOTE\n# Preprocess the training data\nX_train_preprocessed = preprocessor.fit_transform(X_train_2)\n\n# Apply SMOTE\nX_resampled, y_resampled = SMOTE(random_state=42).fit_resample(X_train_preprocessed, y_train)\n\n# Train the XGBoost model with best parameters\nfinal_xgb_model = XGBClassifier(**best_xgb_params)\nfinal_xgb_model.fit(X_resampled, y_resampled)\n\n# Preprocess the test data\nX_test_preprocessed = preprocessor.transform(X_test_2)\n\n# Predict on the test data\nxgb_prob_test_2 = final_xgb_model.predict_proba(X_test_preprocessed)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Image - ViT","metadata":{}},{"cell_type":"code","source":"root_dir = '/kaggle/input/isic-2024-challenge'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T18:02:34.391625Z","iopub.execute_input":"2024-12-14T18:02:34.391933Z","iopub.status.idle":"2024-12-14T18:02:34.396763Z","shell.execute_reply.started":"2024-12-14T18:02:34.391896Z","shell.execute_reply":"2024-12-14T18:02:34.395485Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms\n\ndef remove_hair(image):\n    \"\"\"\n    Remove hair artifacts from an image using the DullRazor approach.\n    \"\"\"\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n\n    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (7, 7))\n    blackhat = cv2.morphologyEx(gray, cv2.MORPH_BLACKHAT, kernel)\n    _, thresh = cv2.threshold(blackhat, 12, 255, cv2.THRESH_BINARY)\n    inpainted = cv2.inpaint(image, thresh, inpaintRadius=1, flags=cv2.INPAINT_TELEA)\n\n    return inpainted","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T18:02:34.398195Z","iopub.execute_input":"2024-12-14T18:02:34.398558Z","iopub.status.idle":"2024-12-14T18:02:39.816077Z","shell.execute_reply.started":"2024-12-14T18:02:34.398524Z","shell.execute_reply":"2024-12-14T18:02:39.814821Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"target_data = train_data[['isic_id', 'target']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T18:02:39.817569Z","iopub.execute_input":"2024-12-14T18:02:39.818294Z","iopub.status.idle":"2024-12-14T18:02:39.833053Z","shell.execute_reply.started":"2024-12-14T18:02:39.818242Z","shell.execute_reply":"2024-12-14T18:02:39.831800Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"print(target_data.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T18:02:39.834553Z","iopub.execute_input":"2024-12-14T18:02:39.835039Z","iopub.status.idle":"2024-12-14T18:02:39.842415Z","shell.execute_reply.started":"2024-12-14T18:02:39.834972Z","shell.execute_reply":"2024-12-14T18:02:39.841261Z"}},"outputs":[{"name":"stdout","text":"(401059, 2)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport os\n\nclass CassavaDataset(torch.utils.data.Dataset):\n    \"\"\"\n    Helper Class to create the PyTorch dataset\n    \"\"\"\n\n    def __init__(self, df, data_path=f'{root_dir}/train-image/image/', secondary_data_path=\"/content/isic-2024-challenge/isic-2020-dataset/train/malignant\", mode=\"train\", transforms=None):\n        super().__init__()\n        self.df_data = df.values\n        self.data_path = data_path\n        self.secondary_data_path = secondary_data_path\n        self.transforms = transforms\n        self.mode = mode\n\n    def __len__(self):\n        return len(self.df_data)\n\n    def __getitem__(self, index):\n        img_name, label = self.df_data[index]\n        if img_name.startswith(\"ISIC2020_\"):\n            stripped_name = img_name.replace(\"ISIC2020_\", \"\")\n            img_path = os.path.join(self.secondary_data_path, f\"{stripped_name}\")\n        else:\n            img_path = os.path.join(self.data_path, f\"{img_name}.jpg\")\n        img = Image.open(img_path).convert(\"RGB\")\n\n        if self.transforms is not None:\n            img = self.transforms(img)\n\n        return img, label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T18:02:39.843743Z","iopub.execute_input":"2024-12-14T18:02:39.844202Z","iopub.status.idle":"2024-12-14T18:02:39.859208Z","shell.execute_reply.started":"2024-12-14T18:02:39.844168Z","shell.execute_reply":"2024-12-14T18:02:39.857777Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class HairRemovalTransform:\n    def __call__(self, img):\n        img_np = np.array(img)\n        img_np = remove_hair(img_np)\n        img = Image.fromarray(img_np)\n        return img\n\nIMG_SIZE = 224\n\ntransforms_train = transforms.Compose(\n    [\n        HairRemovalTransform(),\n        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n        transforms.RandomHorizontalFlip(p=0.3),\n        transforms.RandomVerticalFlip(p=0.3),\n        transforms.RandomResizedCrop(IMG_SIZE),\n        transforms.ToTensor(),\n        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n    ]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T18:02:39.860486Z","iopub.execute_input":"2024-12-14T18:02:39.860813Z","iopub.status.idle":"2024-12-14T18:02:39.877425Z","shell.execute_reply.started":"2024-12-14T18:02:39.860781Z","shell.execute_reply":"2024-12-14T18:02:39.876155Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import timm\n\n\nclass ViTBase16(nn.Module):\n    def __init__(self, n_classes, pretrained=False, model_path=None):\n        super(ViTBase16, self).__init__()\n        # self.model = timm.create_model(\"vit_small_patch16_224\", pretrained=False)\n        self.model = timm.create_model(\"vit_large_patch16_224\", pretrained=False)\n        if pretrained and model_path:\n            self.model.load_state_dict(torch.load(model_path))\n        self.model.head = nn.Linear(self.model.head.in_features, n_classes)\n\n    def forward(self, x):\n        x = self.model(x)\n        return torch.sigmoid(x)\n\n    def train_one_epoch(self, train_loader, criterion, optimizer, device):\n        self.model.train()\n        epoch_loss = 0.0\n        epoch_accuracy = 0.0\n\n        for i, (data, target) in enumerate(train_loader):\n            data, target = data.to(device), target.to(device)\n\n            optimizer.zero_grad()\n            output = self(data)\n            loss = criterion(output, target)\n            loss.backward()\n            accuracy = (output.argmax(dim=1) == target).float().mean()\n\n            epoch_loss += loss.item()\n            epoch_accuracy += accuracy.item()\n\n            optimizer.step()\n            if device.type == \"xla\" and i % 20 == 0:\n                xm.master_print(f\"\\tBATCH {i+1}/{len(train_loader)} - LOSS: {loss.item():.4f}\")\n\n        avg_loss = epoch_loss / len(train_loader)\n        avg_accuracy = epoch_accuracy / len(train_loader)\n        return avg_loss, avg_accuracy\n\n    def validate_one_epoch(self, valid_loader, criterion, device):\n        self.model.eval()\n        valid_loss = 0.0\n        valid_accuracy = 0.0\n\n        with torch.no_grad():\n            for data, target in valid_loader:\n                data, target = data.to(device), target.to(device)\n\n                output = self(data)\n                loss = criterion(output, target)\n\n                accuracy = (output.argmax(dim=1) == target).float().mean()\n\n                valid_loss += loss.item()\n                valid_accuracy += accuracy.item()\n\n        avg_loss = valid_loss / len(valid_loader)\n        avg_accuracy = valid_accuracy / len(valid_loader)\n        return avg_loss, avg_accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T18:02:39.879022Z","iopub.execute_input":"2024-12-14T18:02:39.879471Z","iopub.status.idle":"2024-12-14T18:02:41.839172Z","shell.execute_reply.started":"2024-12-14T18:02:39.879423Z","shell.execute_reply":"2024-12-14T18:02:41.837939Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"from io import BytesIO\nclass HDF5CassavaDataset(torch.utils.data.Dataset):\n\n    def __init__(self, df, hdf5_loader, mode=\"test\", transforms=None):\n        super().__init__()\n        self.df = df\n        self.hdf5_loader = hdf5_loader\n        self.mode = mode\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        isic_id = row['isic_id']\n        # img_bytes = self.hdf5_loader.__getitem__(isic_id)\n        img_bytes = self.hdf5_loader.get_img(isic_id)\n        img = Image.open(BytesIO(img_bytes)).convert(\"RGB\")\n\n        if self.transforms:\n            img = self.transforms(img)\n\n        if self.mode in [\"train\", \"valid\"]:\n            label = row['target']\n            return img, label\n        else: \n            return img, isic_id ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T18:02:41.840685Z","iopub.execute_input":"2024-12-14T18:02:41.841184Z","iopub.status.idle":"2024-12-14T18:02:41.850291Z","shell.execute_reply.started":"2024-12-14T18:02:41.841136Z","shell.execute_reply":"2024-12-14T18:02:41.848935Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# dataloader\nclass UltDataLoader():\n    def __init__(self, data_path, batch_size=32):\n        self.data_path = data_path\n        self.batch_size = batch_size\n        self.index = 0\n\n    def load_meta(self):\n        for _ in ['train', 'test']:\n            target_path = f\"{self.data_path}/{_}-metadata.csv\"\n            setattr(self, f\"{_}_meta\", pd.read_csv(target_path,low_memory=False))\n            \n    def load_hdf5(self):\n        for _ in ['train', 'test']:\n            target_path = f\"{self.data_path}/{_}-image.hdf5\"\n            setattr(self, f\"{_}_hdf5\", h5py.File(target_path, 'r'))\n\n\n    def __getitem__(self, isic_id):\n        if self.train_hdf5.get(isic_id):\n            # seeking from train at first\n            return self.train_hdf5[isic_id][()]\n        elif self.test_hdf5.get(isic_id):\n            # seeking from test if isic_id doesn't exit in train\n            return self.test_hdf5[isic_id][()]\n        else:\n            raise ValueError(f\"ISIC_ID {isic_id} is not found.\")\n\n    def get_img(self, isic_id, test=True):\n        if test:\n            # seeking from train at first\n            return self.test_hdf5[isic_id][()]\n        else:\n            # seeking from test if isic_id doesn't exit in train\n            return self.train_hdf5[isic_id][()]\n    \n    def hdf5_dataset2img(self, isic_id):\n        \"\"\"\n        - We can access image data with `dataset[()]`.\n        - dataset -> hdf5 dataset object\n        - hdf5 dataset object -> We can extract with isic_id as key from h5py.File(path/to/file.hdf5, 'r')\n          e.g) h5py_file[idic_id]\n        \n        Example:\n            >>> import h5py\n            >>> from io import BytesIO\n            >>> from PIL import Image\n            >>> import matplotlib.pyplot as plt\n            >>> with h5py.File('/kaggle/input/isic-2024-challenge/train-image.hdf5') as f:\n            >>>    plt.imshow(Image.open(BytesIO(f['ISIC_0015670'][()])))\n        \"\"\"\n        img_bytes = self.__getitem__(isic_id)\n        return BytesIO(img_bytes)\n\n        \n\ntransforms_valid = transforms.Compose(\n    [\n        HairRemovalTransform(),\n        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n    ]\n)\n\ntransforms_test = transforms.Compose([\n    HairRemovalTransform(),\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T18:02:41.851547Z","iopub.execute_input":"2024-12-14T18:02:41.851877Z","iopub.status.idle":"2024-12-14T18:02:41.874844Z","shell.execute_reply.started":"2024-12-14T18:02:41.851845Z","shell.execute_reply":"2024-12-14T18:02:41.873698Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nimport h5py\n\n\nhdf5_loader = UltDataLoader(root_dir)\nhdf5_loader.load_meta()\nhdf5_loader.load_hdf5()\ntest_meta = hdf5_loader.test_meta\n\ntrain_dataset = CassavaDataset(target_data, transforms=transforms_train)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=8)\n\n# valid_dataset = CassavaDataset(val_meta, transforms=transforms_valid)\n# valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, num_workers=8)\n\ntest_dataset = HDF5CassavaDataset(test_meta, hdf5_loader, mode=\"test\", transforms=transforms_test)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T18:02:41.876436Z","iopub.execute_input":"2024-12-14T18:02:41.876784Z","iopub.status.idle":"2024-12-14T18:02:49.655984Z","shell.execute_reply.started":"2024-12-14T18:02:41.876750Z","shell.execute_reply":"2024-12-14T18:02:49.654876Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"model = ViTBase16(n_classes=2, pretrained=True)\nmd_path0 = '/kaggle/input/best-model-4/best_model_forth.pth'\nmd_path1 = '/kaggle/input/best-model-5/best_model_fifth.pth'\n# checkpoint = torch.load(md_path0)\ncheckpoint = torch.load(md_path1)\nmodel.load_state_dict(checkpoint)\n# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n# criterion = nn.CrossEntropyLoss()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# train_loss, train_acc = model.train_one_epoch(train_loader, criterion, optimizer, device)\n# valid_loss, valid_acc = model.validate_one_epoch(valid_loader, criterion, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T18:02:49.665727Z","iopub.execute_input":"2024-12-14T18:02:49.666095Z","iopub.status.idle":"2024-12-14T18:03:03.157185Z","shell.execute_reply.started":"2024-12-14T18:02:49.666062Z","shell.execute_reply":"2024-12-14T18:03:03.155987Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_24/2525441830.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(md_path0, map_location=torch.device('cpu'))\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"ViTBase16(\n  (model): VisionTransformer(\n    (patch_embed): PatchEmbed(\n      (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n      (norm): Identity()\n    )\n    (pos_drop): Dropout(p=0.0, inplace=False)\n    (patch_drop): Identity()\n    (norm_pre): Identity()\n    (blocks): Sequential(\n      (0): Block(\n        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n          (q_norm): Identity()\n          (k_norm): Identity()\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): Identity()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (norm): Identity()\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): Identity()\n        (drop_path2): Identity()\n      )\n      (1): Block(\n        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n          (q_norm): Identity()\n          (k_norm): Identity()\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): Identity()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (norm): Identity()\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): Identity()\n        (drop_path2): Identity()\n      )\n      (2): Block(\n        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n          (q_norm): Identity()\n          (k_norm): Identity()\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): Identity()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (norm): Identity()\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): Identity()\n        (drop_path2): Identity()\n      )\n      (3): Block(\n        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n          (q_norm): Identity()\n          (k_norm): Identity()\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): Identity()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (norm): Identity()\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): Identity()\n        (drop_path2): Identity()\n      )\n      (4): Block(\n        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n          (q_norm): Identity()\n          (k_norm): Identity()\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): Identity()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (norm): Identity()\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): Identity()\n        (drop_path2): Identity()\n      )\n      (5): Block(\n        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n          (q_norm): Identity()\n          (k_norm): Identity()\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): Identity()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (norm): Identity()\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): Identity()\n        (drop_path2): Identity()\n      )\n      (6): Block(\n        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n          (q_norm): Identity()\n          (k_norm): Identity()\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): Identity()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (norm): Identity()\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): Identity()\n        (drop_path2): Identity()\n      )\n      (7): Block(\n        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n          (q_norm): Identity()\n          (k_norm): Identity()\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): Identity()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (norm): Identity()\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): Identity()\n        (drop_path2): Identity()\n      )\n      (8): Block(\n        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n          (q_norm): Identity()\n          (k_norm): Identity()\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): Identity()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (norm): Identity()\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): Identity()\n        (drop_path2): Identity()\n      )\n      (9): Block(\n        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n          (q_norm): Identity()\n          (k_norm): Identity()\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): Identity()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (norm): Identity()\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): Identity()\n        (drop_path2): Identity()\n      )\n      (10): Block(\n        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n          (q_norm): Identity()\n          (k_norm): Identity()\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): Identity()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (norm): Identity()\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): Identity()\n        (drop_path2): Identity()\n      )\n      (11): Block(\n        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n          (q_norm): Identity()\n          (k_norm): Identity()\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): Identity()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (norm): Identity()\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): Identity()\n        (drop_path2): Identity()\n      )\n      (12): Block(\n        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n          (q_norm): Identity()\n          (k_norm): Identity()\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): Identity()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (norm): Identity()\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): Identity()\n        (drop_path2): Identity()\n      )\n      (13): Block(\n        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n          (q_norm): Identity()\n          (k_norm): Identity()\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): Identity()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (norm): Identity()\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): Identity()\n        (drop_path2): Identity()\n      )\n      (14): Block(\n        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n          (q_norm): Identity()\n          (k_norm): Identity()\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): Identity()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (norm): Identity()\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): Identity()\n        (drop_path2): Identity()\n      )\n      (15): Block(\n        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n          (q_norm): Identity()\n          (k_norm): Identity()\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): Identity()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (norm): Identity()\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): Identity()\n        (drop_path2): Identity()\n      )\n      (16): Block(\n        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n          (q_norm): Identity()\n          (k_norm): Identity()\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): Identity()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (norm): Identity()\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): Identity()\n        (drop_path2): Identity()\n      )\n      (17): Block(\n        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n          (q_norm): Identity()\n          (k_norm): Identity()\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): Identity()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (norm): Identity()\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): Identity()\n        (drop_path2): Identity()\n      )\n      (18): Block(\n        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n          (q_norm): Identity()\n          (k_norm): Identity()\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): Identity()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (norm): Identity()\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): Identity()\n        (drop_path2): Identity()\n      )\n      (19): Block(\n        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n          (q_norm): Identity()\n          (k_norm): Identity()\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): Identity()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (norm): Identity()\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): Identity()\n        (drop_path2): Identity()\n      )\n      (20): Block(\n        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n          (q_norm): Identity()\n          (k_norm): Identity()\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): Identity()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (norm): Identity()\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): Identity()\n        (drop_path2): Identity()\n      )\n      (21): Block(\n        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n          (q_norm): Identity()\n          (k_norm): Identity()\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): Identity()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (norm): Identity()\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): Identity()\n        (drop_path2): Identity()\n      )\n      (22): Block(\n        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n          (q_norm): Identity()\n          (k_norm): Identity()\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): Identity()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (norm): Identity()\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): Identity()\n        (drop_path2): Identity()\n      )\n      (23): Block(\n        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n          (q_norm): Identity()\n          (k_norm): Identity()\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): Identity()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (norm): Identity()\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): Identity()\n        (drop_path2): Identity()\n      )\n    )\n    (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n    (fc_norm): Identity()\n    (head_drop): Dropout(p=0.0, inplace=False)\n    (head): Linear(in_features=1024, out_features=2, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"from torch.nn.functional import softmax","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T18:03:03.158619Z","iopub.execute_input":"2024-12-14T18:03:03.159093Z","iopub.status.idle":"2024-12-14T18:03:03.164968Z","shell.execute_reply.started":"2024-12-14T18:03:03.159050Z","shell.execute_reply":"2024-12-14T18:03:03.163705Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import h5py\nimport matplotlib.pyplot as plt\nimport os\nimport random\nimport numpy as np\nfrom PIL import Image\nfrom torchvision import transforms\nimport pandas as pd\nimport kagglehub\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport timm\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nimport time\nimport torchvision.models as models\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as transforms\nfrom sklearn.metrics import accuracy_score, classification_report\nimport io\nfrom io import BytesIO\nimport cv2\nfrom torchvision import transforms\nimport shutil\n\nroot_dir = '/kaggle/input/isic-2024-challenge'\nmodel_v2 = models.mobilenet_v2(weights=None)\nmodel_v2.classifier[1] = torch.nn.Linear(in_features=1280, out_features=1) \nfor param in model_v2.parameters():\n    param.requires_grad = False\n\ncheckpoint_path = \"/kaggle/input/mobilenet-v2/model_checkpoint_v2.pth\"\ncheckpoint = torch.load(checkpoint_path)\n\nmodel_v2.load_state_dict(checkpoint['model_state_dict'])\n\nmodel_v2.eval()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel_v2.to(device)\n\nprobabilities_mobilenet = []\n\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs = inputs.to(device)\n        outputs = model_v2(inputs).squeeze()\n        probs = torch.sigmoid(outputs)\n        probabilities_mobilenet.extend(probs.cpu().numpy())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_features(model, dataloader, device, test=False):\n    probabilities = []\n    with torch.no_grad():  # No need to calculate gradients during inference\n        for data, target in dataloader:\n            # print(data)\n            # print(target)\n            data = data.to(device)\n            if not test:\n                target = target.to(device)  # Send target to device if necessary\n            outputs = model(data)\n        \n            # Convert logits to probabilities using softmax\n            probs = softmax(outputs, dim=1)  # Shape: [batch_size, n_classes]\n            # print(probs)\n            probabilities.extend(probs.cpu().numpy())\n            \n    return probabilities","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T18:03:03.166612Z","iopub.execute_input":"2024-12-14T18:03:03.167070Z","iopub.status.idle":"2024-12-14T18:03:03.188038Z","shell.execute_reply.started":"2024-12-14T18:03:03.167023Z","shell.execute_reply":"2024-12-14T18:03:03.187030Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"# Ensemble ","metadata":{}},{"cell_type":"code","source":"import pandas.api.types\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\n\nclass ParticipantVisibleError(Exception):\n    pass\n\n\ndef score(solution, submission, min_tpr: float=0.80) -> float:\n\n    # rescale the target. set 0s to 1s and 1s to 0s (since sklearn only has max_fpr)\n    v_gt = abs(np.asarray(solution)-1)\n\n    # flip the submissions to their compliments\n    v_pred = -1.0*np.asarray(submission)\n\n    max_fpr = abs(1-min_tpr)\n\n    # using sklearn.metric functions: (1) roc_curve and (2) auc\n    fpr, tpr, _ = roc_curve(v_gt, v_pred, sample_weight=None)\n    if max_fpr is None or max_fpr == 1:\n        return auc(fpr, tpr)\n    if max_fpr <= 0 or max_fpr > 1:\n        raise ValueError(\"Expected min_tpr in range [0, 1), got: %r\" % min_tpr)\n\n    # Add a single point at max_fpr by linear interpolation\n    stop = np.searchsorted(fpr, max_fpr, \"right\")\n    x_interp = [fpr[stop - 1], fpr[stop]]\n    y_interp = [tpr[stop - 1], tpr[stop]]\n    tpr = np.append(tpr[:stop], np.interp(max_fpr, x_interp, y_interp))\n    fpr = np.append(fpr[:stop], max_fpr)\n    partial_auc = auc(fpr, tpr)\n\n    return(partial_auc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T07:49:58.412939Z","iopub.execute_input":"2024-12-16T07:49:58.413773Z","iopub.status.idle":"2024-12-16T07:49:58.420619Z","shell.execute_reply.started":"2024-12-16T07:49:58.413738Z","shell.execute_reply":"2024-12-16T07:49:58.419669Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Ensure the model is on the correct device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\n# xgb_prob_train = xgb_ml.predict_proba(X_train_data)\n# vit_prob_train = extract_features(model, train_loader, device)\n\nxgb_prob_test = xgb_ml.predict_proba(X_test_data)\nvit_prob_test = extract_features(model, test_loader, device, test=True)\n# Combine features\n# train_combined_prob = np.hstack([vit_prob_train, xgb_prob_train])\n# test_combined_prob = np.hstack([vit_prob_test, xgb_prob_test])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T18:03:03.189764Z","iopub.execute_input":"2024-12-14T18:03:03.190238Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"from scipy.optimize import minimize\n\n# xgb_prob_train_1 = xgb_prob_train[:,1]\n# vit_prob_train_1 = np.array([prob[1] for prob in vit_prob_train])\n\npred1 = xgb_prob_test[:,1]\npred2 = xgb_prob_test_2[:,1]\npred3 = np.array([prob[1] for prob in vit_prob_test])\npred4 = np.array(probabilities_mobilenet)\npreds = [pred1,pred2,pred3,pred4]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict\n# final_preds = np.sum(result.x.reshape(-1, 1) * prob_test_1, axis=0)\n\nfinal_preds = (0.3*pred1+0.3*pred2+0.3*pred3+0.1*pred4)/4\n# final_preds = np.power(pred1*pred2*pred3, 1/3)\n# Logit Average\n# Convert probabilities to log-odds\n# log_odds = [np.log(p / (1 - p)) for p in preds]\n\n# # Average the log-odds\n# average_log_odds = np.mean(log_odds, axis=0)\n\n# # Convert back to probabilities\n# final_preds = 1 / (1 + np.exp(-average_log_odds))\n\nsubmission = pd.DataFrame(\n    {'isic_id': X_test['isic_id'], 'target': final_preds},\n    columns = ['isic_id', 'target'])\nsubmission.to_csv('/kaggle/working/submission.csv', index = False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}